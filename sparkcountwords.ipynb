{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGAcEzS4f8+s6cjyl2t4Jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gelsa45/Cloud-Computing-Lab-S7DS/blob/main/sparkcountwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Z1J1nHTE_I",
        "outputId": "7a5f85e6-fc6d-4146-e3a3-32f4e22303a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.s\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2025-10-25 17:01:43--  https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz.1’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  19.7MB/s    in 22s     \n",
            "\n",
            "2025-10-25 17:02:06 (17.5 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz.1’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark 3.5.0 with Hadoop 3\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyXM7MFgTP9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "826a3c97",
        "outputId": "11f6b48a-c25e-47d0-b3da-6b0ea450dd9b"
      },
      "source": [
        "!pip install findspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffb65b36"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a5788c",
        "outputId": "8fd2e180-8cb2-4597-a18c-dc364c4c8102"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"SPARK_HOME:\", os.environ.get(\"SPARK_HOME\"))\n",
        "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPARK_HOME: /usr/local/lib/python3.12/dist-packages/pyspark\n",
            "JAVA_HOME: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "644027ca"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/openjdk-11-jdk-headless\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext('local','appname')\n",
        "\n",
        "# Step 4: Read text file into an RDD\n",
        "text_rdd = sc.textFile(\"file.txt\")  # Replace \"file.txt\" with your file path\n",
        "\n",
        "# Step 5: Split each line into words\n",
        "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Step 6: Map each word to a tuple (word, 1)\n",
        "pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
        "\n",
        "# Step 7: Reduce by key (sum counts of each word)\n",
        "counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Step 8: Collect results and print\n",
        "for word, count in counts_rdd.collect():\n",
        "    print(word, count)\n",
        "\n",
        "# Step 9: Stop SparkSession\n",
        "sc.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YoxVIspUzd7",
        "outputId": "11887780-0a3c-4444-f961-d9ad9d0e84c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This 1\n",
            "is 1\n",
            "a 1\n",
            "test 1\n",
            "file. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57f40a1"
      },
      "source": [
        "# Create a dummy text file for the word count example\n",
        "with open(\"file.txt\", \"w\") as f:\n",
        "    f.write(\"This is a test file.\\n\")\n",
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}